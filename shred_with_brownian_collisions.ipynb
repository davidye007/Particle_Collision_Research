{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpLcT955+RhlcKP/CPP4vG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidye007/Particle_Collision_Research/blob/main/shred_with_brownian_collisions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### install packages\n",
        "%%capture\n",
        "### changes\n",
        "!pip install opencv-python\n",
        "!pip install seaborn\n",
        "!pip install scipy\n",
        "!pip install process-data\n",
        "!pip install pytorch_forecasting"
      ],
      "metadata": {
        "id": "Eff-PmQ_f0rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### import packages\n",
        "import numpy as np\n",
        "import pytorch_forecasting\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from dataclasses import make_dataclass\n",
        "import pdb\n",
        "from processdata import load_data\n",
        "from processdata import TimeSeriesDataset\n",
        "import models\n",
        "import torch\n",
        "import scipy.io\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "VyQXcjes8vew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch video\n",
        "vid = cv2.VideoCapture(\"Granular Brownian Motion Square.mp4\")\n",
        "# attempt to extract first frame\n",
        "success, frame = vid.read()\n",
        "# create dataframe for storing particle center coordinates and frame number\n",
        "df = pd.DataFrame(columns = ['x','y','r','frame'])\n",
        "# set counter to frame zero\n",
        "counter = 0;\n",
        "# max frame count is 10\n",
        "max = 100000\n",
        "while (success and counter < max):\n",
        "  # convert frame to greyscale\n",
        "  gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "  # Blur using 3 * 3 kernel\n",
        "  gray_blurred = cv2.blur(gray, (3, 3))\n",
        "  # Apply Hough transform on blurred frame\n",
        "  detected_circles = cv2.HoughCircles(gray_blurred,cv2.HOUGH_GRADIENT, dp = 1, minDist = 8, param1 = 50,\n",
        "                                    param2 = 9, minRadius = 3, maxRadius = 5)\n",
        "  # Check if circle detected\n",
        "  if detected_circles is not None:\n",
        "    # Round circle parameters x, y and r to integers\n",
        "    detected_circles = np.uint16(np.around(detected_circles))\n",
        "    for circle in detected_circles[0, :]:\n",
        "       x, y, r = circle[0], circle[1], circle[2]\n",
        "       df.loc[df.shape[0]] = [x, y, r, counter] \n",
        "       # draw circumfrence of detected circle\n",
        "       # cv2.circle(frame, (np.uint16(np.around(x)), np.uint16(np.around(y))), r, (255, 0, 255), 2)\n",
        "       # draw center of detected circle (for verification)\n",
        "       cv2.circle(frame, (x, y), radius = 1, color = (0, 0, 255), thickness = 2)\n",
        "  else:\n",
        "    print(\"no circles detected\")\n",
        "  # mark boundaries for random sensor location placewment with green box\n",
        "  # xlim (10,690) and ylim (10,710)\n",
        "  cv2.line(frame, (10, 10), (690, 10), color = (0,255,0), thickness = 1)\n",
        "  cv2.line(frame, (690, 10), (690, 710), color = (0,255,0), thickness = 1)\n",
        "  cv2.line(frame, (10, 10), (10, 710), color = (0,255,0), thickness = 1)\n",
        "  cv2.line(frame, (10, 710), (690, 710), color = (0,255,0), thickness = 1)\n",
        "  # show detected particles (for verification)\n",
        "  # cv2_imshow(frame)\n",
        "  # select next frame\n",
        "  success, frame = vid.read()\n",
        "  # update counter\n",
        "  counter = counter + 1\n",
        "  if (counter == max):\n",
        "      print('reached max frames')"
      ],
      "metadata": {
        "id": "FMqC6edDgTY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tempfile import TemporaryFile\n",
        "detected = TemporaryFile()\n",
        "np.save(detected, df)"
      ],
      "metadata": {
        "id": "C0BVavIUQHWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed for reproducibility\n",
        "random.seed(20)\n",
        "# number of sensors\n",
        "num_sensors = 3\n",
        "# randomly select three sensor locations\n",
        "sensor_locations = [(random.randint(10, 690), random.randint(10, 710)) for i in range(num_sensors)]\n",
        "# matrix Q of densities: shape(num of frames, num of sensors)\n",
        "Q = np.zeros((counter,num_sensors))\n",
        "for i in range(counter):\n",
        "  single_frame = df[df[\"frame\"] == i]\n",
        "  x = single_frame[\"x\"].astype(float)\n",
        "  y = single_frame[\"y\"].astype(float)\n",
        "  # (row: x,y|column: every ball)\n",
        "  values = np.vstack((x.ravel(), y.ravel()))\n",
        "  kernel = stats.gaussian_kde(values, bw_method=0.3)\n",
        "  for j in range(num_sensors):\n",
        "    Q[i,j] = kernel.pdf(sensor_locations[j])"
      ],
      "metadata": {
        "id": "8iHPdtUvksmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed for reproducibility\n",
        "random.seed(20)\n",
        "num_sensors = 3 \n",
        "lags = 52\n",
        "load_X = Q\n",
        "# timesteps: n\n",
        "n = load_X.shape[0]\n",
        "# sensors: m\n",
        "m = load_X.shape[1]\n",
        "# train indicies column vector: choose 1000 timesteps from (total_timesteps_n - lags) timesteps\n",
        "train_indices = np.random.choice(n - lags, size=1000, replace=False)\n",
        "# column vector of ones with size (total_timesteps_n - lags)\n",
        "mask = np.ones(n - lags)\n",
        "# change elements corresponding to train_indicies to zero\n",
        "mask[train_indices] = 0\n",
        "# validation and test set: mask indicies that are non-zero\n",
        "valid_test_indices = np.arange(0, n - lags)[np.where(mask!=0)[0]]\n",
        "# 0,2,4,6... end\n",
        "valid_indices = valid_test_indices[::2]\n",
        "# 1,3,5,7... end\n",
        "test_indices = valid_test_indices[1::2]\n",
        "### tansform all data\n",
        "sc = MinMaxScaler()\n",
        "sc = sc.fit(load_X[train_indices])\n",
        "transformed_X = sc.transform(load_X)\n",
        "\n",
        "### Generate input sequences to a SHRED model (3D matrix of (total_time_steps - lags, lags, num_sensors))\n",
        "all_data_in = np.zeros((n - lags, lags, num_sensors))\n",
        "### for all total_time_steps - lags\n",
        "for i in range(len(all_data_in)):\n",
        "    ### for each time step, lags: look from i to 52 time steps into the future, sensor = sensor_locations\n",
        "    all_data_in[i] = transformed_X[i:i+lags, sensor_locations]\n",
        "\n",
        "### Generate training validation and test datasets both for reconstruction of states and forecasting sensors\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "train_data_in = torch.tensor(all_data_in[train_indices], dtype=torch.float32).to(device)\n",
        "valid_data_in = torch.tensor(all_data_in[valid_indices], dtype=torch.float32).to(device)\n",
        "test_data_in = torch.tensor(all_data_in[test_indices], dtype=torch.float32).to(device)\n",
        "\n",
        "### -1 to have output be at the same time as final sensor measurements\n",
        "### start of prediction is the same element as end of sensor measurement\n",
        "train_data_out = torch.tensor(transformed_X[train_indices + lags - 1], dtype=torch.float32).to(device)\n",
        "valid_data_out = torch.tensor(transformed_X[valid_indices + lags - 1], dtype=torch.float32).to(device)\n",
        "test_data_out = torch.tensor(transformed_X[test_indices + lags - 1], dtype=torch.float32).to(device)\n",
        "\n",
        "train_dataset = TimeSeriesDataset(train_data_in, train_data_out)\n",
        "valid_dataset = TimeSeriesDataset(valid_data_in, valid_data_out)\n",
        "test_dataset = TimeSeriesDataset(test_data_in, test_data_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "0xGuustW9OfR",
        "outputId": "02ba961a-682e-4b72-c388-10b06358fc5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-722df026c87d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# train indicies column vector: choose 1000 timesteps from (total_timesteps_n - lags) timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# column vector of ones with size (total_timesteps_n - lags)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: a must be greater than 0 unless no samples are taken"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shred = models.SHRED(num_sensors, m, hidden_size=64, hidden_layers=2, l1=350, l2=400, dropout=0.1).to(device)\n",
        "validation_errors = models.fit(shred, train_dataset, valid_dataset, batch_size=64, num_epochs=1000, lr=1e-3, verbose=True, patience=5)"
      ],
      "metadata": {
        "id": "9ZgyYjDGp3-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_recons = sc.inverse_transform(shred(test_dataset.X).detach().cpu().numpy())\n",
        "test_ground_truth = sc.inverse_transform(test_dataset.Y.detach().cpu().numpy())\n",
        "print(np.linalg.norm(test_recons - test_ground_truth) / np.linalg.norm(test_ground_truth))"
      ],
      "metadata": {
        "id": "omY9QXscp5g3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}